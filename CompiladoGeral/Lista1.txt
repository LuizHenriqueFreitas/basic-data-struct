## Análise Assintótica da Função `exemplo1`

A função `exemplo1` simplesmente imprime os números de $0$ a $n-1$. Para analisar seu tempo de execução, precisamos contar as operações em relação ao valor de `n`.

Vamos analisar o laço de repetição:

* **Laço `for`:** `for (int i = 0; i < n; i++)`
    * Este laço é executado $n$ vezes.
    * Dentro do laço, temos as seguintes operações para cada iteração:
        * Comparação: `i < n`
        * Incremento: `i++`
        * Impressão: `cout << i << endl;` (esta operação pode levar um tempo variável dependendo do sistema, mas para fins de análise assintótica, consideramos um tempo constante por caractere ou um tempo proporcional ao número de dígitos de $i$).

Para simplificar a análise assintótica, podemos considerar que as operações dentro do laço (comparação, incremento e impressão) levam um tempo constante. Vamos chamar esse tempo constante de $c_1$.

Assim, o tempo total gasto pelas operações dentro do laço será $n \times c_1$.

Além disso, temos operações fora do laço, como a inicialização de `i` e a própria chamada da função. Vamos considerar que essas operações levam um tempo constante $c_2$.

### Função de Tempo de Execução

A função que descreve o tempo de execução, $T(n)$, pode ser expressa como:

$$T(n) = c_1 \cdot n + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada iteração do laço.
* $c_2$ representa o tempo gasto pelas operações de inicialização e finalização da função.

### Complexidade Assintótica

Na **análise assintótica**, estamos interessados no comportamento da função de tempo de execução quando $n$ se torna muito grande. Nesse cenário, o termo de maior ordem na função de tempo é o mais significativo, e as constantes e termos de menor ordem podem ser ignorados.

No caso de $T(n) = c_1 \cdot n + c_2$, o termo de maior ordem é $c_1 \cdot n$. Ignorando a constante $c_1$, obtemos $n$.

Portanto, a **complexidade assintótica** da função `exemplo1` é **linear**.

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(n)$$

Isso significa que o tempo de execução da função `exemplo1` cresce linearmente com o tamanho da entrada $n$. Se $n$ dobrar, o tempo de execução aproximadamente dobrará.

---

## Análise Assintótica da Função `exemplo2`

A função `exemplo2` imprime pares de números de $(0,0)$ até $(n-1, n-1)$. Para determinar seu tempo de execução, precisamos analisar os laços aninhados e as operações internas em relação ao valor de `n`.

Vamos analisar os laços de repetição:

* **Laço externo:** `for (int i = 0; i < n; i++)`
    * Este laço é executado `n` vezes.

* **Laço interno:** `for (int j = 0; j < n; j++)`
    * Para *cada* iteração do laço externo, este laço interno também é executado `n` vezes.
    * Dentro do laço interno, temos as seguintes operações para cada iteração:
        * Comparação: `j < n`
        * Incremento: `j++`
        * Impressão: `cout << i << j << endl;`

O número total de vezes que a operação de impressão (e as outras operações internas) é executada é o produto do número de iterações do laço externo pelo número de iterações do laço interno: $n \times n = n^2$.

Podemos considerar que as operações dentro do laço mais interno (comparação, incremento e impressão) levam um tempo constante. Vamos chamar esse tempo constante de $c_1$.

Assim, o tempo total gasto pelas operações dentro do laço interno será $n^2 \times c_1$.

Além disso, temos operações fora dos laços, como a inicialização da função e das variáveis. Vamos considerar que essas operações levam um tempo constante $c_2$.

### Função de Tempo de Execução

A função que descreve o tempo de execução, $T(n)$, pode ser expressa como:

$$T(n) = c_1 \cdot n^2 + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada execução das operações mais internas.
* $c_2$ representa o tempo gasto pelas operações de inicialização e finalização da função.

### Complexidade Assintótica

Na **análise assintótica**, focamos no termo de maior ordem da função de tempo de execução quando $n$ se torna muito grande, ignorando constantes e termos de menor ordem.

No caso de $T(n) = c_1 \cdot n^2 + c_2$, o termo de maior ordem é $c_1 \cdot n^2$. Ignorando a constante $c_1$, obtemos $n^2$.

Portanto, a **complexidade assintótica** da função `exemplo2` é **quadrática**.

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(n^2)$$

Isso significa que o tempo de execução da função `exemplo2` cresce **quadraticamente** com o tamanho da entrada $n$. Se $n$ dobrar, o tempo de execução aproximadamente quadruplicará.

---

## Análise Assintótica da Função `exemplo3`

A função `exemplo3` imprime potências de 2 até um valor menor que `n`. Para analisar seu tempo de execução, precisamos entender como o laço de repetição se comporta em relação ao valor de `n`.

Vamos analisar o laço `for`:

* **Inicialização:** `int i = 1;`
* **Condição:** `i < n;`
* **Incremento/Atualização:** `i *= 2;` (o valor de `i` dobra a cada iteração)

O valor de `i` assume os seguintes valores: $1, 2, 4, 8, \dots, 2^k$. O laço continua enquanto $2^k < n$.

Para encontrar o número de iterações, precisamos resolver $2^k < n$ para $k$.
Aplicando o logaritmo base 2 em ambos os lados:

$$\log_2(2^k) < \log_2(n)$$
$$k < \log_2(n)$$

Como $k$ representa o número de iterações (começando de $k=0$ para $i=1$), o número de iterações é aproximadamente $\log_2(n)$.

Dentro do laço, temos as seguintes operações para cada iteração:
* Comparação: `i < n`
* Multiplicação: `i *= 2`
* Impressão: `cout << i << endl;`

Consideramos que essas operações dentro do laço levam um tempo constante $c_1$.

Assim, o tempo total gasto pelas operações dentro do laço será aproximadamente $\log_2(n) \times c_1$.

Operações fora do laço (inicialização da função e da variável `i`) levam um tempo constante $c_2$.

### Função de Tempo de Execução

A função que descreve o tempo de execução, $T(n)$, pode ser expressa como:

$$T(n) = c_1 \cdot \log_2(n) + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada iteração do laço.
* $c_2$ representa o tempo gasto pelas operações de inicialização e finalização da função.

### Complexidade Assintótica

Na **análise assintótica**, focamos no termo de maior ordem da função de tempo de execução quando $n$ se torna muito grande, ignorando constantes e termos de menor ordem.

No caso de $T(n) = c_1 \cdot \log_2(n) + c_2$, o termo de maior ordem é $c_1 \cdot \log_2(n)$. Ignorando a constante $c_1$ e a base do logaritmo (já que $\log_b(n) = \frac{\log_c(n)}{\log_c(b)}$, onde $\log_c(b)$ é uma constante), obtemos $\log(n)$.

Portanto, a **complexidade assintótica** da função `exemplo3` é **logarítmica**.

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(\log n)$$

Isso significa que o tempo de execução da função `exemplo3` cresce **logaritmicamente** com o tamanho da entrada $n$. Isso é um crescimento muito eficiente; por exemplo, se $n$ quadruplicar, o tempo de execução aumentará em apenas uma constante (duas iterações a mais, pois $\log_2(4n) = \log_2(4) + \log_2(n) = 2 + \log_2(n)$).

---

## Análise Assintótica da Função `estranho1`

A função `estranho1` possui laços aninhados onde o laço interno depende do valor atual do iterador do laço externo, que por sua vez cresce exponencialmente. Vamos analisar o comportamento de cada laço.

### Laço Externo: `for (int i = 1; i < n; i *= 3)`

* A variável `i` começa em $1$ e é multiplicada por $3$ a cada iteração.
* Os valores de `i` serão: $1, 3, 9, 27, \dots, 3^k$.
* O laço continua enquanto $3^k < n$. Para encontrar o número de iterações ($k$), aplicamos o logaritmo base $3$:
    $$k < \log_3(n)$$
* Portanto, o laço externo executa aproximadamente $\log_3(n)$ vezes.

### Laço Interno: `for (int j = 0; j < i * i; j++)`

* Para cada iteração do laço externo, o laço interno executa $i \times i$ vezes (ou $i^2$ vezes).
* À medida que `i` assume os valores $3^0, 3^1, 3^2, \dots, 3^k$, o número de execuções do laço interno será:
    * Quando $i = 3^0 = 1$: o laço interno executa $1^2 = 1$ vez.
    * Quando $i = 3^1 = 3$: o laço interno executa $3^2 = 9$ vezes.
    * Quando $i = 3^2 = 9$: o laço interno executa $9^2 = 81$ vezes.
    * ...
    * Quando $i = 3^k$: o laço interno executa $(3^k)^2 = 3^{2k}$ vezes.

### Total de Operações (`contador++`)

O número total de vezes que `contador++` é executado é a soma das execuções do laço interno para cada valor de `i`:

$$1^2 + 3^2 + (3^2)^2 + \dots + (3^k)^2$$$$= (3^0)^2 + (3^1)^2 + (3^2)^2 + \dots + (3^k)^2$$$$= 3^0 + 3^2 + 3^4 + \dots + 3^{2k}$$

Esta é uma soma de uma série geométrica com primeiro termo $a = 3^0 = 1$, razão $r = 3^2 = 9$, e número de termos $k+1$.
A soma de uma série geométrica é dada por $\frac{a(r^{m} - 1)}{r - 1}$, onde $m$ é o número de termos. Aqui, $m = k+1$.

Soma = $\frac{1 \cdot (9^{k+1} - 1)}{9 - 1} = \frac{9^{k+1} - 1}{8}$

Sabemos que $k \approx \log_3(n)$, o que significa $3^k \approx n$.
Então, $9^{k+1} = 9 \cdot 9^k = 9 \cdot (3^2)^k = 9 \cdot (3^k)^2 \approx 9 \cdot n^2$.

Substituindo na soma:
Soma $\approx \frac{9 \cdot n^2 - 1}{8} \approx \frac{9}{8}n^2$

Considerando que as operações dentro do laço mais interno (incremento de `contador` e as comparações/incrementos dos laços) levam um tempo constante $c_1$, e as operações fora dos laços levam um tempo constante $c_2$, a função de tempo de execução pode ser expressa.

### Função de Tempo de Execução

$$T(n) = c_1 \cdot \left(\frac{9}{8}n^2\right) + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada execução das operações mais internas.
* $c_2$ representa o tempo gasto pelas operações de inicialização e finalização da função.

### Complexidade Assintótica

Para a **análise assintótica**, focamos no termo de maior ordem da função $T(n)$ quando $n$ se torna muito grande, ignorando constantes e termos de menor ordem.

No caso de $T(n) = c_1 \cdot \left(\frac{9}{8}n^2\right) + c_2$, o termo dominante é $c_1 \cdot \frac{9}{8}n^2$. Ignorando as constantes $c_1$ e $\frac{9}{8}$, obtemos $n^2$.

Portanto, a **complexidade assintótica** da função `estranho1` é **quadrática**.

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(n^2)$$

Isso significa que o tempo de execução da função `estranho1` cresce **quadraticamente** com o tamanho da entrada $n$. Apesar de o laço externo ser logarítmico, o laço interno que depende de $i^2$ (onde $i$ cresce exponencialmente) domina a complexidade.

---

## Análise Assintótica da Função `estranho2`

A função `estranho2` possui um laço `while` com um incremento condicional da variável `i`. Precisamos analisar como `i` cresce em relação a `n` para determinar o número de iterações do laço.

### Laço `while`: `while (i < n)`

A variável `i` começa em $0$ e é incrementada de forma condicional:
* Se `i` for par, `i` é incrementado em $3$.
* Se `i` for ímpar, `i` é incrementado em $2$.

Vamos observar a sequência de valores de `i`:

* **Início:** $i = 0$ (par)
* **1ª iteração:** $i = 0 + 3 = 3$ (ímpar)
* **2ª iteração:** $i = 3 + 2 = 5$ (ímpar)
* **3ª iteração:** $i = 5 + 2 = 7$ (ímpar)
* **4ª iteração:** $i = 7 + 2 = 9$ (ímpar)
* **...**
* **Após a primeira iteração, `i` se torna 3. A partir daí, `i` sempre será ímpar, pois um número ímpar + 2 (ou + 3 se fosse par) resultará sempre em um número ímpar.**

Seja $k$ o número de iterações.
* Na primeira iteração, `i` vai de $0$ para $3$.
* Nas iterações subsequentes, `i` é incrementado de $2$ em $2$.

Então, após a primeira iteração, o valor de `i` se comportará como $3 + 2 \times (k-1)$, onde $(k-1)$ é o número de iterações adicionais.

O laço para quando `i` atinge ou excede `n`. Podemos aproximar o crescimento de `i` como linear. Embora o incremento varie entre $2$ e $3$, a média de incremento é constante (próxima de $2.5$ se considerarmos que o primeiro passo é o único diferente de $2$).

Para que `i` atinja `n`, o número de iterações $k$ será aproximadamente proporcional a `n`.

$$i \approx 2.5 \cdot k$$$$n \approx 2.5 \cdot k$$$$k \approx \frac{n}{2.5}$$

Isso significa que o número de iterações é diretamente proporcional a `n`.

Dentro do laço, temos as seguintes operações para cada iteração:
* Comparação: `i < n`
* Operação de módulo: `i % 2`
* Comparação: `i % 2 == 0`
* Operador ternário para escolher o incremento
* Adição: `i += ...`

Todas essas operações dentro do laço podem ser consideradas de tempo constante, digamos $c_1$.

Operações fora do laço (inicialização da função e da variável `i`) levam um tempo constante $c_2$.

### Função de Tempo de Execução

A função que descreve o tempo de execução, $T(n)$, pode ser expressa como:

$$T(n) = c_1 \cdot \left(\frac{n}{2.5}\right) + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada iteração do laço.
* $c_2$ representa o tempo gasto pelas operações de inicialização e finalização da função.

### Complexidade Assintótica

Na **análise assintótica**, focamos no termo de maior ordem da função de tempo de execução quando $n$ se torna muito grande, ignorando constantes e termos de menor ordem.

No caso de $T(n) = c_1 \cdot \left(\frac{n}{2.5}\right) + c_2$, o termo dominante é $c_1 \cdot \frac{n}{2.5}$. Ignorando as constantes $c_1$ e $\frac{1}{2.5}$, obtemos $n$.

Portanto, a **complexidade assintótica** da função `estranho2` é **linear**.

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(n)$$

Isso significa que o tempo de execução da função `estranho2` cresce **linearmente** com o tamanho da entrada $n$. Apesar do incremento variar um pouco, o número de passos para `i` atingir `n` é diretamente proporcional a `n`.

---

## Análise Assintótica da Função `estranho3`

A função `estranho3` apresenta um laço `for` com duas variáveis de controle que se movem em direções opostas: `i` dobra a cada iteração, e `j` é dividida por dois a cada iteração. O laço continua enquanto `i < j`.

Vamos analisar o comportamento das variáveis `i` e `j`:

* **Variável `i`:** Começa em $1$ e é multiplicada por $2$ a cada iteração. Os valores de `i` serão $1, 2, 4, 8, \dots, 2^k$.
* **Variável `j`:** Começa em `n` e é dividida por $2$ a cada iteração. Os valores de `j` serão $n, n/2, n/4, n/8, \dots, n/2^k$.

O laço continua enquanto $i < j$. Ou seja, $2^k < n/2^k$.
Vamos resolver essa inequação para encontrar o número de iterações ($k$):

$$2^k < \frac{n}{2^k}$$Multiplicando ambos os lados por $2^k$:$$2^k \cdot 2^k < n$$
$$2^{2k} < n$$

Agora, aplicamos o logaritmo base $2$ em ambos os lados:
$$\log_2(2^{2k}) < \log_2(n)$$$$2k < \log_2(n)$$$$k < \frac{\log_2(n)}{2}$$

Isso significa que o número de iterações do laço é proporcional a $\frac{1}{2} \log_2(n)$, que é o mesmo que $\log_2(n)$ em termos de complexidade assintótica.

Dentro do laço, temos as seguintes operações para cada iteração:
* Comparação: `i < j`
* Multiplicação: `i *= 2`
* Divisão: `j /= 2`
* Impressão: `cout << i << " " << j << endl;`

Todas essas operações são consideradas de tempo constante. Vamos chamar esse tempo constante de $c_1$.

Operações fora do laço (inicialização da função e das variáveis `i` e `j`) levam um tempo constante $c_2$.

### Função de Tempo de Execução

A função que descreve o tempo de execução, $T(n)$, pode ser expressa como:

$$T(n) = c_1 \cdot \left(\frac{\log_2(n)}{2}\right) + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada iteração do laço.
* $c_2$ representa o tempo gasto pelas operações de inicialização e finalização da função.

### Complexidade Assintótica

Na **análise assintótica**, nos concentramos no termo de maior ordem da função de tempo de execução quando $n$ se torna muito grande. Constantes e termos de menor ordem são ignorados.

No caso de $T(n) = c_1 \cdot \left(\frac{\log_2(n)}{2}\right) + c_2$, o termo dominante é $c_1 \cdot \frac{\log_2(n)}{2}$. Ignorando as constantes $c_1$ e $\frac{1}{2}$ (e a base do logaritmo), obtemos $\log(n)$.

Portanto, a **complexidade assintótica** da função `estranho3` é **logarítmica**.

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(\log n)$$

Isso demonstra que o algoritmo é muito eficiente, pois o tempo de execução cresce de forma logarítmica com o tamanho da entrada `n`. Mesmo para valores muito grandes de `n`, o número de iterações permanece relativamente pequeno.

---

## Análise Assintótica da Função `estranho4`

A função `estranho4` busca por um elemento `x` em um array `v` de tamanho `n`. O interessante aqui é a forma como a variável `i` (o índice) é incrementada dentro do laço `while`: `i += i / 2 + 1;`. Isso sugere um crescimento que não é linear nem puramente exponencial.

### Laço `while`: `while (i < n)`

* **Inicialização:** `i = 0`.
* **Condição de Parada:** `i < n`.
* **Incremento:** `i = i + (i / 2) + 1`.

Vamos analisar a sequência de valores de `i` para entender seu crescimento:

* Se $i = 0$, $i$ se torna $0 + 0/2 + 1 = 1$.
* Se $i = 1$, $i$ se torna $1 + 1/2 + 1 = 1 + 0 + 1 = 2$ (considerando divisão inteira).
* Se $i = 2$, $i$ se torna $2 + 2/2 + 1 = 2 + 1 + 1 = 4$.
* Se $i = 3$, $i$ se torna $3 + 3/2 + 1 = 3 + 1 + 1 = 5$.
* Se $i = 4$, $i$ se torna $4 + 4/2 + 1 = 4 + 2 + 1 = 7$.
* Se $i = 5$, $i$ se torna $5 + 5/2 + 1 = 5 + 2 + 1 = 8$.
* Se $i = 6$, $i$ se torna $6 + 6/2 + 1 = 6 + 3 + 1 = 10$.
* Se $i = 7$, $i$ se torna $7 + 7/2 + 1 = 7 + 3 + 1 = 11$.
* Se $i = 8$, $i$ se torna $8 + 8/2 + 1 = 8 + 4 + 1 = 13$.

Observe que o incremento `i / 2 + 1` faz com que `i` cresça de forma que se aproxima de `i + i/2 = 1.5 * i`. Ou seja, `i` cresce aproximadamente $1.5$ vezes a cada passo. Isso é um crescimento exponencial, mas com uma base de $1.5$ em vez de $2$.

Para um crescimento $i_{k+1} \approx 1.5 \cdot i_k$, o número de passos $k$ para $i$ atingir `n` pode ser encontrado resolvendo $1.5^k \approx n$.

Aplicando o logaritmo em ambos os lados:
$$\log(1.5^k) \approx \log(n)$$$$k \cdot \log(1.5) \approx \log(n)$$$$k \approx \frac{\log(n)}{\log(1.5)}$$

Como $\frac{1}{\log(1.5)}$ é uma constante, o número de iterações é proporcional a $\log(n)$.

Dentro do laço, temos as seguintes operações para cada iteração:
* Comparação: `i < n`
* Acesso ao array: `v[i]`
* Comparação: `v[i] == x`
* Operação de divisão inteira: `i / 2`
* Soma: `i / 2 + 1`
* Atribuição: `i += ...`
* Potencial `return true;`

Todas essas operações são consideradas de tempo constante, digamos $c_1$.

Operações fora do laço (inicialização da função e das variáveis, e o `return false` final) levam um tempo constante $c_2$.

### Função de Tempo de Execução

A função que descreve o tempo de execução, $T(n)$, pode ser expressa como:

$$T(n) = c_1 \cdot \frac{\log(n)}{\log(1.5)} + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada iteração do laço.
* $c_2$ representa o tempo gasto pelas operações de inicialização e finalização da função.

### Complexidade Assintótica

Na **análise assintótica**, focamos no termo de maior ordem da função de tempo de execução quando `n` se torna muito grande, ignorando constantes e termos de menor ordem.

No caso de $T(n) = c_1 \cdot \frac{\log(n)}{\log(1.5)} + c_2$, o termo dominante é $c_1 \cdot \frac{\log(n)}{\log(1.5)}$. Ignorando as constantes $c_1$ e $\frac{1}{\log(1.5)}$ (e a base do logaritmo, já que $\log_b(n)$ é o mesmo que $\log(n)$ em notação Big O), obtemos $\log(n)$.

Portanto, a **complexidade assintótica** da função `estranho4` é **logarítmica**.

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(\log n)$$

Isso significa que, apesar de o incremento parecer "estranho", ele ainda resulta em um crescimento logarítmico para a variável `i`, tornando o algoritmo bastante eficiente para grandes valores de `n`. O número de acessos ao array e de comparações é proporcional ao logaritmo do tamanho do array.

---

## Análise Assintótica da Função `estranho5`

A função `estranho5` contém laços aninhados. O laço externo itera linearmente, enquanto o laço interno itera logaritmicamente em relação ao valor do iterador do laço externo.

### Laço Externo: `for (int i = 1; i < n; i++)`

* Este laço executa $n-1$ vezes. Ele itera com `i` assumindo valores de $1$ até $n-1$.

### Laço Interno: `for (int j = 1; j < i; j *= 2)`

* Este laço é executado para cada valor de `i` do laço externo.
* A variável `j` começa em $1$ e é dobrada a cada iteração, parando quando `j` for maior ou igual a `i`.
* O número de iterações do laço interno para um dado `i` é o número de vezes que precisamos multiplicar $1$ por $2$ até que o resultado seja próximo de `i`. Isso é análogo a encontrar $k$ tal que $2^k \approx i$.
* Portanto, o número de iterações do laço interno para um dado `i` é aproximadamente $\log_2(i)$.

### Total de Operações (`soma += j`)

O número total de vezes que `soma += j` é executado é a soma das execuções do laço interno para cada valor de `i` do laço externo:

$$\sum_{i=1}^{n-1} \log_2(i)$$

Para analisar essa soma assintoticamente, podemos aproximá-la. A função $\log_2(i)$ é uma função crescente.
Para uma soma de $\log(i)$ de $1$ a $n-1$, podemos aproximar usando a integral:

$$\int_{1}^{n} \log_2(x) \, dx$$

A integral de $\ln(x)$ (que é $\log_e(x)$) é $x \ln(x) - x$. Como $\log_2(x) = \frac{\ln(x)}{\ln(2)}$, a integral de $\log_2(x)$ é:

$$\frac{1}{\ln(2)} \int_{1}^{n} \ln(x) \, dx = \frac{1}{\ln(2)} [x \ln(x) - x]_{1}^{n}$$$$= \frac{1}{\ln(2)} [(n \ln(n) - n) - (1 \ln(1) - 1)]$$$$= \frac{1}{\ln(2)} [n \ln(n) - n + 1]$$

Sabemos que $\frac{n \ln(n)}{\ln(2)} = n \log_2(n)$.

Portanto, a soma é aproximadamente $n \log_2(n) - n + \frac{1}{\ln(2)}$.

Considerando que as operações dentro do laço mais interno (soma de `j` a `soma`, e as operações dos laços) levam um tempo constante $c_1$, e as operações fora dos laços levam um tempo constante $c_2$, a função de tempo de execução pode ser expressa.

### Função de Tempo de Execução

$$T(n) = c_1 \cdot (n \log_2 n) + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada execução da operação `soma += j` e das operações dos laços.
* $c_2$ representa o tempo gasto pelas operações de inicialização e finalização da função.

### Complexidade Assintótica

Na **análise assintótica**, focamos no termo de maior ordem da função de tempo de execução quando $n$ se torna muito grande, ignorando constantes e termos de menor ordem.

No caso de $T(n) = c_1 \cdot (n \log_2 n) + c_2$, o termo dominante é $c_1 \cdot n \log_2 n$. Ignorando a constante $c_1$ e a base do logaritmo (pois $\log_b(n)$ é equivalente a $\log(n)$ em notação Big O), obtemos $n \log n$.

Portanto, a **complexidade assintótica** da função `estranho5` é **linearítmica** (também conhecida como log-linear).

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(n \log n)$$

Isso significa que o tempo de execução da função `estranho5` cresce ligeiramente mais rápido que linear, mas muito mais lentamente que quadrático. Algoritmos com essa complexidade são considerados eficientes para muitos problemas.

---

## Análise Assintótica da Função `estranho6`

A função `estranho6` itera sobre um array `v` de tamanho `n`, mas o incremento do índice `i` dentro do laço `while` é condicional: `i` aumenta em $3$ se `v[i]` for par, e em $2$ se `v[i]` for ímpar.

### Laço `while`: `while (i < n)`

* **Inicialização:** `i = 0`.
* **Condição de Parada:** `i < n`.
* **Incremento:** `i` aumenta em $2$ ou $3$ a cada iteração.

O valor de `i` cresce a cada iteração. No melhor caso (todos os elementos são pares), `i` aumentará em $3$ a cada passo. No pior caso (todos os elementos são ímpares), `i` aumentará em $2$ a cada passo. Na maioria dos casos, o incremento será uma mistura de $2$s e $3$s, resultando em um incremento médio de aproximadamente $2.5$.

Seja $k$ o número de iterações. Para que `i` atinja `n`, o número de iterações será aproximadamente:

* **Melhor caso:** $k \approx n/3$ (quando `i` sempre aumenta $3$)
* **Pior caso:** $k \approx n/2$ (quando `i` sempre aumenta $2$)
* **Caso médio:** $k \approx n/2.5$ (quando `i` aumenta em média $2.5$)

Em todos os cenários, o número de iterações é diretamente proporcional a `n`.

Dentro do laço, temos as seguintes operações para cada iteração:
* Acesso ao array: `v[i]`
* Operação de módulo: `v[i] % 2`
* Comparação: `v[i] % 2 == 0`
* Adição: `i += 3` ou `i += 2`
* Comparação do laço: `i < n`

Todas essas operações são consideradas de **tempo constante**. Vamos chamá-lo de $c_1$.

A operação de inicialização da função e da variável `i` fora do laço leva um tempo constante $c_2$.

### Função de Tempo de Execução

A função que descreve o tempo de execução, $T(n)$, pode ser expressa como:

$$T(n) = c_1 \cdot k + c_2$$

Onde $k$ é o número de iterações. Como $k$ é proporcional a $n$ (ou seja, $k \approx n/C$, onde $C$ é uma constante entre $2$ e $3$), podemos reescrever:

$$T(n) = c_1 \cdot \left(\frac{n}{C}\right) + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada iteração do laço.
* $C$ é uma constante que reflete o incremento médio de `i` (entre $2$ e $3$).
* $c_2$ representa o tempo gasto pelas operações de inicialização da função.

### Complexidade Assintótica

Na **análise assintótica**, estamos interessados no comportamento da função de tempo de execução quando `n` se torna muito grande. Ignoramos as constantes e os termos de menor ordem.

No caso de $T(n) = c_1 \cdot \left(\frac{n}{C}\right) + c_2$, o termo dominante é $c_1 \cdot \frac{n}{C}$. Ignorando as constantes $c_1$ e $1/C$, obtemos $n$.

Portanto, a **complexidade assintótica** da função `estranho6` é **linear**.

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(n)$$

Isso significa que o tempo de execução da função `estranho6` cresce **linearmente** com o tamanho da entrada `n`. Mesmo com a variação no incremento de `i`, o número total de passos para `i` atingir `n` continua sendo diretamente proporcional a `n`.

---

## Análise Assintótica da Função `estranho7`

A função `estranho7` itera sobre uma matriz quadrada (ou uma porção dela, dependendo de `n` e do tamanho fixo de 100 colunas). O laço externo percorre as linhas, e o laço interno percorre as colunas, mas a direção do percorrimento do laço interno alterna a cada linha.

### Laço Externo: `for (int i = 0; i < n; i++)`

* Este laço é executado $n$ vezes, uma vez para cada linha da matriz (ou até `n` linhas se `n` for menor ou igual a 100).

### Laço Interno: `for (int j = (i % 2 == 0 ? 0 : n - 1); j >= 0 && j < n; j += (i % 2 == 0 ? 1 : -1))`

* Para cada iteração do laço externo (cada linha `i`), o laço interno percorre uma **linha completa**.
* **Condição de inicialização de `j`:**
    * Se `i` for par (`i % 2 == 0`), `j` começa em $0$.
    * Se `i` for ímpar (`i % 2 != 0`), `j` começa em $n-1$.
* **Condição de parada do laço `j`:** `j >= 0 && j < n`. Isso garante que `j` permaneça dentro dos limites válidos da coluna.
* **Incremento/Decremento de `j`:**
    * Se `i` for par, `j` incrementa em $1$ (`j += 1`).
    * Se `i` for ímpar, `j` decrementa em $1$ (`j += -1`).

Independentemente da direção (para frente ou para trás), o laço interno sempre percorre **$n$ elementos** para cada linha. Ou seja, ele executa $n$ vezes.

Dentro do laço interno, as operações principais são:
* Acesso à matriz: `mat[i][j]`
* Impressão: `cout << mat[i][j] << " "`
* Operações de `j`: inicialização, comparação, incremento/decremento.

Todas essas operações são consideradas de **tempo constante**. Vamos chamá-lo de $c_1$.

A operação de impressão de nova linha (`cout << endl;`) após cada linha do laço externo também leva um tempo constante. A inicialização da função e do laço externo levam um tempo constante $c_2$.

### Função de Tempo de Execução

O número total de operações (especificamente as operações dentro do laço mais interno) é o produto do número de iterações do laço externo pelo número de iterações do laço interno:

Total de operações = (Número de iterações do laço `i`) $\times$ (Número de iterações do laço `j` para cada `i`)
Total de operações = $n \times n = n^2$

Portanto, a função de tempo de execução, $T(n)$, pode ser expressa como:

$$T(n) = c_1 \cdot n^2 + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada operação de acesso e impressão dentro do laço aninhado, além das operações de controle dos laços.
* $c_2$ representa o tempo gasto pelas operações de inicialização da função e a impressão de nova linha.

### Complexidade Assintótica

Na **análise assintótica**, estamos interessados no termo de maior ordem da função de tempo de execução quando $n$ se torna muito grande, ignorando constantes e termos de menor ordem.

No caso de $T(n) = c_1 \cdot n^2 + c_2$, o termo dominante é $c_1 \cdot n^2$. Ao ignorar a constante $c_1$, obtemos $n^2$.

Portanto, a **complexidade assintótica** da função `estranho7` é **quadrática**.

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(n^2)$$

Isso significa que o tempo de execução da função `estranho7` cresce **quadraticamente** com o tamanho da entrada `n`. Mesmo com a alternância na direção de percorrimento das colunas, cada linha ainda é percorrida completamente, resultando em um tempo de execução proporcional a $n \times n$.

---

## Análise Assintótica da Função `estranho8`

A função `estranho8` calcula uma soma de incrementos baseada em laços aninhados. O laço externo itera linearmente, mas o limite do laço interno depende do iterador do laço externo, especificamente `n / i`.

### Laço Externo: `for (int i = 1; i <= n; i++)`

* Este laço executa $n$ vezes, com `i` variando de $1$ a $n$.

### Laço Interno: `for (int j = 1; j <= n / i; j++)`

* Para cada iteração do laço externo (cada valor de `i`), o laço interno executa $\lfloor n/i \rfloor$ vezes (arredondando para baixo, pois `j` e `n` são inteiros).
* Os valores de `j` irão de $1$ até $n/i$.

### Total de Operações (`soma++`)

O número total de vezes que `soma++` é executado é a soma das execuções do laço interno para cada valor de `i` do laço externo:

$$\sum_{i=1}^{n} \frac{n}{i}$$

Podemos fatorar $n$ para fora da soma:

$$n \sum_{i=1}^{n} \frac{1}{i}$$

A soma $\sum_{i=1}^{n} \frac{1}{i}$ é a **série harmônica**, que é aproximada por $\ln(n) + \gamma$, onde $\gamma$ é a constante de Euler-Mascheroni (aproximadamente $0.577$). Para grandes valores de $n$, o termo dominante é $\ln(n)$.

Portanto, o número total de operações é aproximadamente $n \cdot \ln(n)$.

Considerando que as operações dentro do laço mais interno (incremento de `soma`, e as operações de controle dos laços) levam um tempo constante $c_1$, e as operações fora dos laços levam um tempo constante $c_2$, a função de tempo de execução pode ser expressa.

### Função de Tempo de Execução

$$T(n) = c_1 \cdot (n \ln n) + c_2$$

Onde:
* $c_1$ representa o tempo gasto por cada execução da operação `soma++` e das operações dos laços.
* $c_2$ representa o tempo gasto pelas operações de inicialização e finalização da função.

### Complexidade Assintótica

Na **análise assintótica**, focamos no termo de maior ordem da função de tempo de execução quando $n$ se torna muito grande, ignorando constantes e termos de menor ordem.

No caso de $T(n) = c_1 \cdot (n \ln n) + c_2$, o termo dominante é $c_1 \cdot n \ln n$. Ignorando a constante $c_1$ e lembrando que $\ln n$ e $\log n$ são equivalentes em termos de notação Big O (pois $\ln n = \log_e n = \frac{\log_2 n}{\log_2 e}$), obtemos $n \log n$.

Portanto, a **complexidade assintótica** da função `estranho8` é **linearítmica** (ou log-linear).

Em termos da **notação Big O**, a complexidade é:

$$\mathcal{O}(n \log n)$$

Isso indica que o tempo de execução da função `estranho8` cresce ligeiramente mais rápido que linear, mas é significativamente mais eficiente que um crescimento quadrático. Algoritmos com essa complexidade são frequentemente encontrados em algoritmos de ordenação eficientes, por exemplo.